{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUozjvnVVK_l"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import jaccard_score\n",
        "import numpy as np\n",
        "\n",
        "# Define categorical test results\n",
        "jack = ['P', 'N', 'N', 'A']\n",
        "mary = ['P', 'A', 'P', 'N']\n",
        "jim  = ['N', 'N', 'N', 'A']\n",
        "\n",
        "# Define a mapping from categories to integers\n",
        "label_mapping = {'P': 0, 'N': 1, 'A': 2}\n",
        "\n",
        "# Convert characters to numerical values\n",
        "jack_encoded = [label_mapping[x] for x in jack]\n",
        "mary_encoded = [label_mapping[x] for x in mary]\n",
        "jim_encoded  = [label_mapping[x] for x in jim]\n",
        "\n",
        "# Convert to binary arrays (one-hot encoded style for multiclass Jaccard)\n",
        "def to_binary_matrix(encoded_list, num_classes=3):\n",
        "    return np.eye(num_classes)[encoded_list]\n",
        "\n",
        "jack_bin = to_binary_matrix(jack_encoded)\n",
        "mary_bin = to_binary_matrix(mary_encoded)\n",
        "jim_bin  = to_binary_matrix(jim_encoded)\n",
        "\n",
        "# Flatten and compute Jaccard similarity for each pair\n",
        "def compute_jaccard(a_bin, b_bin):\n",
        "    a_flat = a_bin.flatten()\n",
        "    b_flat = b_bin.flatten()\n",
        "    return jaccard_score(a_flat, b_flat, average='macro')\n",
        "\n",
        "print(\"Jaccard(Jack, Mary):\", round(compute_jaccard(jack_bin, mary_bin), 3))\n",
        "print(\"Jaccard(Jack, Jim):\", round(compute_jaccard(jack_bin, jim_bin), 3))\n",
        "print(\"Jaccard(Jim, Mary):\", round(compute_jaccard(jim_bin, mary_bin), 3))\n"
      ]
    }
  ]
}