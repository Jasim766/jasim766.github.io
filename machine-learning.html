<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Machine Learning | Jasim Alzaabi</title>
  <link rel="stylesheet" href="assets/css/style.css" />
</head>
<body>
  <header class="hero smaller">
    <div class="container">
      <h1>Machine Learning E-Portfolio</h1>
      <nav class="navbar">
        <a href="index.html">Home</a>
        <a href="about.html">About</a>
        <a href="machine-learning.html" class="active">Machine Learning</a>
        <a href="research-methods.html">Research Methods</a>
      </nav>
    </div>
  </header>

  <section class="section">
    <div class="container">
     <h2>Unit 1 ‚Äì Introduction to Machine Learning</h2>

<h4> Summary of Learning</h4>
<p>
This first unit provided an introduction to the core principles of machine learning (ML), its relationship with artificial intelligence, and the wider implications of deploying such systems at scale. We discussed how ML is integrated into everyday tools and services, from recommendation engines to automated decision-making. The session also introduced the distinction between Industry 4.0 and Industry 5.0‚Äîhighlighting a shift from automation-focused systems to more human-centric, ethically guided technologies (Metcalf, 2024).
</p>
<p>
A key takeaway from this week was the growing concern around how rapidly implemented ML systems can create unintended risks‚Äîparticularly when deployed without sufficient consideration for privacy, security, and user impact. As technologies become more integrated into industry and society, their design must reflect not only performance metrics, but also values such as fairness, transparency, and resilience.
</p>

<h4>üìÅ Artefact</h4>
<p>
For this unit, I contributed to the discussion forum with a post titled <em>‚ÄúM&S Cyberattack in UK Retail‚Äù</em>. In this post, I analysed the April 2025 cyberattack on Marks & Spencer (M&S) as a case study that illustrates how a narrow focus on speed and automation‚Äîas typical of Industry 4.0‚Äîcan expose serious security and ethical vulnerabilities. I drew on Metcalf‚Äôs (2024) concept of Industry 5.0 to propose how a more adaptive, human-in-the-loop approach could have reduced the scale of disruption and protected consumer trust.  
<a href="https://github.com/your-username/ml-eportfolio/blob/main/unit1_ms_cyberattack_reflection.md" target="_blank">View Discussion Post</a>
</p>

<h4>ü™û Reflection (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)</h4>
<p>
<strong>What:</strong> The unit introduced machine learning as a key driver in both innovation and risk within modern industry. Analysing the M&S incident helped me understand the real-world consequences when ethical design is overlooked in favour of efficiency or automation.<br><br>

<strong>So What:</strong> This case changed my perception of ML systems‚Äîfrom seeing them as purely technical achievements to understanding them as socio-technical systems. The scale of customer impact and reputational loss at M&S highlighted how quickly trust can break down if systems fail to account for human oversight and resilience (Ravikumar & Young, 2025; Gallagher, 2025).<br><br>

<strong>Now What:</strong> Moving forward, I plan to critically evaluate ML architectures not only for performance, but also for ethical integrity, data protection, and risk mitigation. I also want to explore how principles like redundancy and decentralisation could be designed into future systems, in line with Industry 5.0 values.
</p>

<h4> Collaboration</h4>
<p>
In our team formation session, I helped shape our initial group contract by proposing weekly communication via whatsapp and google meet . We agreed to use shared Google Docs for updates and keep regular check-ins.
</p>

<h4> Evidence</h4>
<p>
My forum post, titled ‚ÄúM&S Cyberattack in UK Retail‚Äù, 
  M&S Cyberattack  in UK Retail

 Metcalf talks about how Industry 5.0 is not just about adding more tech or making everything automatic like in Industry 4.0, but more about making systems that think about people, are flexible, and follow ethical ideas(Metcalf, 2024). This is important in areas like retail, where digital changes happened super-fast, but in many cases, they didn‚Äôt really think about how to protect workers or customers properly. A lot of systems were built just to be fast or efficient, not to be fair or safe for everyone involved(Metcalf, 2024).

 One serious case that shows how scary  these systems can be happened in April 2025, when Marks & Spencer (M&S), one of the biggest retail companies in the UK, was hit by a major cyberattack. The attackers got into customer data though payment info wasn‚Äôt stolen and caused big problems with online shopping and even systems inside physical stores (Ravikumar &  Young, 2025). Experts said the company was losing around ¬£25 to ¬£30 million every week, especially in its clothing and home departments. On top of that, M&S‚Äôs share price dropped sharply by about 15% which meant they lost around ¬£1.3 billion in market value (Gallagher, 2025).

 What happened at M&S shows the downside of how Industry 4.0 has been applied‚Äîfocusing too much on speed, automation, and storing everything in one place, without really thinking about what happens if things go wrong (Heuser,  & B., & Wang, L., 2021). Metcalf‚Äôs view of Industry 5.0 goes in a different direction. It‚Äôs more about making systems that can adapt, involve human decisions, and are built to be strong against unexpected problems. If M&S had already been using a setup where AI systems worked with human teams, had tools to spot risks early, and didn‚Äôt rely on just one central system, maybe the damage wouldn‚Äôt have been so massive (Metcalf, 2024).

  The attack seriously damaged customer trust in M&S.  their customers shared their frustration and fear on forums and Trustpilot, mainly about personal data being exposed. This supports research showing that loyalty depends heavily on data security and ethical tech use (Mittelstadt, & Floridi,, 2017)
  
</p>

<h4>üõ†Ô∏è Skills & Competencies</h4>
<ul>
  <li>Developed a foundational understanding of machine learning applications and challenges</li>
  <li>Critically analysed real-world incident using Industry 5.0 theory</li>
  <li>Contributed to team collaboration and communication strategy</li>
</ul>

<h4>References</h4>
<ul>
  <li>Metcalf, G.S. (2024) <em>An Introduction to Industry 5.0: History, Foundations, and Futures</em>. SpringerLink.</li>
  <li>Ravikumar, S. and Young, S. (2025) <em>UK‚Äôs M&S says customer data was taken in cyber attack</em>. Reuters.</li>
  <li>Gallagher, R. (2025) <em>M&S Cybersecurity Incident Still Causing Disruption</em>. Bloomberg.</li>
  <li>Mittelstadt, B. and Floridi, L. (2017) <em>Why a Right to Explanation of Automated Decision-Making Does Not Exist in GDPR</em>. International Data Privacy Law.</li>
</ul>

<hr />



<h2>Unit 2 ‚Äì Exploratory Data Analysis</h2>

<h4>Summary of Learning</h4>
<p>
This unit introduced the foundational principles of Exploratory Data Analysis (EDA), focusing on the importance of understanding data structure, identifying anomalies, and preparing datasets for machine learning. We reviewed key concepts such as feature distribution, outlier detection, and correlation analysis, and discussed how EDA helps form the basis for accurate and reliable modelling.
</p>
<p>
One of the core insights from this unit was that raw data is rarely ready for immediate use in machine learning. EDA enables practitioners to clean, visualise, and assess data before moving to algorithm development. This step is essential for both supervised and unsupervised learning, ensuring the quality of inputs and improving downstream model performance.
</p>

<h4>Artefact</h4>
<p>
my artefact is a reflective summary and personal notes taken while engaging with the core reading (Bishop & Bishop, 2024, Chapter 3) and lecturecast. I participated in the forum and group chat discussions on EDA techniques, and shared thoughts on how data anomalies can mislead model interpretation if not properly identified. My reflections focused on real-world risks of neglecting EDA in business-critical applications, such as financial forecasting or fraud detection.
</p>

<h4>Reflection (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)</h4>
<p>
<strong>What:</strong> I learned that EDA is not simply about generating plots or statistics, but about exploring data with intent. It involves questioning the reliability, relevance, and completeness of the dataset before doing any modelling work.<br><br>

<strong>So What:</strong> This changed my perception of ML workflows. I used to think modelling came first, but now I understand how poor EDA leads to overfitting, biased models, or misleading insights. The reading and peer comments reinforced how common data quality issues are, and how easily they are overlooked without structured analysis.<br><br>

<strong>Now What:</strong> For future assignments, I will build in more time for EDA as a formal process. I also plan to use tools like correlation matrices, summary statistics, and visualisation libraries when I begin working on modelling tasks, even if not required, because I now see their long-term value.
</p>

<h4>Collaboration</h4>
<p>
During the week, I contributed to our team discussion around the importance of data validation in machine learning. I shared insights from the reading and lecturecast, particularly around the concept of "garbage in, garbage out," and how this applies to training models on poorly understood datasets. I also reflected on how EDA can reduce risk in practical applications like medical imaging or credit scoring.
</p>



<h4>Skills & Competencies</h4>
<ul>
  <li>Strengthened understanding of data integrity and its role in ML performance</li>
  <li>Engaged with peer learning through reflective discussion</li>
  <li>Linked theory to practical risks in real-world machine learning deployments</li>
</ul>

<h4>References</h4>
<ul>
  <li>Bishop, C. and Bishop, H. (2024). <em>Deep Learning: Foundations and Concepts</em>. Cambridge University Press. Chapter 3.</li>
  <li>Chernozhukov, V., Hansen, C., & Spindler, M. (2024). <em>Applied Causal Inference Powered by ML and AI</em>. Oxford University Press.</li>
  <li>Patil, P. (2018). <em>What is Exploratory Data Analysis?</em></li>
  <li>Harmadi, A. (2021). <em>10 Things to Do When Conducting Your Exploratory Data Analysis (EDA)</em>.</li>
</ul>

<hr />



<h2>Unit 3 ‚Äì Correlation and Regression</h2>

<h4>Summary of Learning</h4>
<p>
This week‚Äôs content introduced the statistical foundations of correlation and regression. Correlation was examined as a means of quantifying the strength and direction of linear associations between two variables, while regression was explored as a method for constructing predictive relationships. These techniques are essential for supervised learning tasks, especially in understanding how variables interact and how future outcomes can be estimated based on input data.
</p>

<h4>Artefact</h4>
<p>
The unit‚Äôs practical exercises were completed through four Jupyter Notebooks: <code>covariance_pearson_correlation.ipynb</code>, <code>linear_regression.ipynb</code>, <code>multiple_linear_regression.ipynb</code>, and <code>polynomial_regression.ipynb</code>. These notebooks covered the implementation of Pearson correlation, simple linear regression, multivariable linear regression, and polynomial curve fitting. The exercises allowed me to visualise relationships, interpret r-values, and apply regression to real-life scenarios using Python libraries.
</p>

<h4>Reflection (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)</h4>
<p><strong>WHAT:</strong> I explored how correlation identifies statistical association and how regression estimates the nature of this relationship. This was contextualised through hands-on application with generated and real-world data.</p>
<p><strong>SO WHAT:</strong> This experience taught me to approach relationships in data critically. I recognised the importance of distinguishing statistical correlation from causality, and I gained insight into when linear regression may or may not be appropriate for predictive purposes.</p>
<p><strong>NOW WHAT:</strong> I plan to apply these concepts more systematically in future projects. For example, regression will become a tool not only for prediction but also for selecting relevant features and validating model assumptions early in the pipeline.</p>

<h4>Collaboration</h4>
<p>
Collaboration during this unit involved structured peer discussions focused on interpreting regression outputs and identifying common pitfalls. We examined cases of multicollinearity and discussed model assumptions such as normality of residuals and homoscedasticity. Feedback from peers helped reinforce best practices and revealed different approaches to visualising and evaluating regression results.
</p>

<h4>Evidence</h4>
<p>
GitHub repository with notebook submissions: <a href="https://github.com/your-repo/ml-unit3-regression" target="_blank">https://github.com/your-repo/ml-unit3-regression</a>. This includes annotated Python notebooks with correlation heatmaps, regression diagnostics, and R¬≤ evaluations.
</p>

<h4>Skills & Competencies</h4>
<ul>
  <li>Understanding of correlation strength and direction using Pearson‚Äôs r</li>
  <li>Development of linear, multiple, and polynomial regression models</li>
  <li>Hands-on use of NumPy, pandas, scikit-learn, matplotlib, and seaborn for statistical modelling</li>
</ul>

<hr />



<h2>Unit 4 ‚Äì Linear Regression with Scikit-Learn</h2>

<h4>üß† Summary of Learning</h4>
<p>
This unit introduced the use of Scikit-Learn for implementing both simple and multiple linear regression models. Emphasis was placed on preparing input features, training regression models, and evaluating them using R¬≤, MAE, and RMSE. We also examined Scikit-Learn‚Äôs modular design and how it fits into an end-to-end ML workflow.
</p>

<h4>üìÅ Artefact</h4>
<p>
Jupyter Notebook on regression modeling with Scikit-Learn using a car price dataset.  
<a href="https://github.com/your-username/ml-eportfolio/blob/main/unit4_linear_regression_scikit.ipynb" target="_blank">View on GitHub</a>
</p>

<h4>ü™û Reflection (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)</h4>
<p>
<strong>WHAT:</strong> I used Scikit-Learn to fit and evaluate linear regression models.<br />
<strong>SO WHAT:</strong> I gained practical knowledge of pipeline construction and model validation.<br />
<strong>NOW WHAT:</strong> I will consistently use `train_test_split`, `LinearRegression`, and validation metrics when developing predictive models in real-world scenarios.
</p>

<h4>ü§ù Collaboration</h4>
<p>
Led a short tutorial on Scikit-Learn‚Äôs regression module during our weekly team call. I also helped standardize our team‚Äôs approach to evaluating linear models.
</p>

<h4>üì∏ Evidence</h4>
<p>
Model outputs, R¬≤ comparisons, and residual plots documented in GitHub notebook. Code and visual results also shared in team repo.
</p>

<h4>üõ†Ô∏è Skills & Competencies</h4>
<ul>
  <li>Used Scikit-Learn to implement linear regression models</li>
  <li>Applied model evaluation techniques (R¬≤, MAE, RMSE)</li>
</ul>

<hr />


<h2>Unit 5 ‚Äì Clustering</h2>

<h4>üß† Summary of Learning</h4>
<p>
This unit explored unsupervised learning, focusing on clustering techniques such as k-means and agglomerative clustering. We studied how clustering groups data points based on similarity measures like Euclidean distance and evaluated clustering quality using metrics like SSE and silhouette score. The broader application of clustering in pattern recognition, image analysis, and business intelligence was also covered.
</p>

<h4>üìÅ Artefact</h4>
<p>
I completed two visual clustering animations (uniform and custom point sets), then reflected on cluster formation logic in the module wiki. I also created a clustering notebook demonstrating k-means clustering using Scikit-Learn.  
<a href="https://github.com/your-username/ml-eportfolio/blob/main/unit5_clustering.ipynb" target="_blank">View on GitHub</a>
</p>

<h4>ü™û Reflection (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)</h4>
<p>
<strong>WHAT:</strong> I applied unsupervised clustering algorithms and evaluated clusters using silhouette scores.<br />
<strong>SO WHAT:</strong> This gave me insight into model-free learning methods, especially the challenge of defining 'optimal' clusters.<br />
<strong>NOW WHAT:</strong> I‚Äôll explore DBSCAN and hierarchical clustering next and use silhouette analysis for cluster evaluation across future datasets.
</p>

<h4>ü§ù Collaboration</h4>
<p>
I commented on two peers‚Äô wiki posts, offering alternate interpretations of cluster boundaries. I also shared my notebook on k-means, which a teammate extended with new data features.
</p>

<h4>üì∏ Evidence</h4>
<p>
Screenshots of animation choices, silhouette graphs, and final clusters are included in appendix. Jupyter Notebook and wiki post are both referenced.
</p>

<h4>üõ†Ô∏è Skills & Competencies</h4>
<ul>
  <li>Applied k-means clustering and evaluated with silhouette scoring</li>
  <li>Compared visual vs. algorithmic cluster assessment strategies</li>
</ul>

<hr />

<h2>Unit 6 ‚Äì Clustering with Python</h2>

<h4>üß† Summary of Learning</h4>
<p>
This unit applied the K-Means clustering algorithm using Scikit-Learn on real-life datasets. We explored the use of clustering to reveal patterns in unlabelled data and interpreted clusters through visualisation and inertia/silhouette metrics.
</p>

<h4>üìÅ Artefact</h4>
<p>
Created a Jupyter Notebook clustering customer data using K-Means. Included elbow plots and cluster interpretation.  
<a href="https://github.com/your-username/ml-eportfolio/blob/main/unit6_kmeans.ipynb" target="_blank">View on GitHub</a>
</p>

<h4>ü™û Reflection (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)</h4>
<p>
<strong>WHAT:</strong> I applied clustering on retail data and evaluated the optimal number of clusters.<br />
<strong>SO WHAT:</strong> It helped me see how unsupervised methods can expose structure and user behaviour.<br />
<strong>NOW WHAT:</strong> I aim to explore clustering evaluation in more depth, comparing DBSCAN and GMM models.
</p>

<h4>ü§ù Collaboration</h4>
<p>
We discussed differences in cluster interpretation. I presented the elbow method and led our visual validation.
</p>

<h4>üì∏ Evidence</h4>
<p>
Elbow chart, cluster scatter plot and silhouette score visual saved in appendix and GitHub.
</p>

<h4>üõ†Ô∏è Skills & Competencies</h4>
<ul>
  <li>Used K-Means for customer segmentation</li>
  <li>Evaluated cluster quality using silhouette score</li>
</ul>

<hr />


<h2>Unit 7 ‚Äì Introduction to Artificial Neural Networks</h2>

<h4>üß† Summary of Learning</h4>
<p>
We explored the structure and function of ANNs, including perceptrons and activation functions. This unit covered biological inspiration, multilayer structures, and their applications across various domains.
</p>

<h4>üìÅ Artefact</h4>
<p>
Built a simple perceptron, AND gate simulation, and a multi-layer perceptron with sigmoid activation using Python.  
<a href="https://github.com/your-username/ml-eportfolio/blob/main/unit7_ann_intro.ipynb" target="_blank">View on GitHub</a>
</p>

<h4>ü™û Reflection (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)</h4>
<p>
<strong>WHAT:</strong> I simulated neural networks from scratch using NumPy.<br />
<strong>SO WHAT:</strong> I grasped the role of non-linear activations in enabling expressive learning.<br />
<strong>NOW WHAT:</strong> I‚Äôll transition to using frameworks like TensorFlow and PyTorch for more scalable ANN implementations.
</p>

<h4>ü§ù Collaboration</h4>
<p>
Shared logic gate results and discussed pros/cons of sigmoid vs ReLU. Team peer-reviewed activation function performance.
</p>

<h4>üì∏ Evidence</h4>
<p>
Logic gate screenshots, weight updates, and loss graphs added to appendix.
</p>

<h4>üõ†Ô∏è Skills & Competencies</h4>
<ul>
  <li>Simulated basic ANN models from scratch</li>
  <li>Understood forward propagation and activations</li>
</ul>

<hr />


<h2>Unit 8 ‚Äì Training an Artificial Neural Network</h2>

<h4>üß† Summary of Learning</h4>
<p>
We explored training neural networks through backpropagation and weight updates. Key topics included error propagation, gradient descent, and adjusting weights through optimisation.
</p>

<h4>üìÅ Artefact</h4>
<p>
Extended the MLP implementation from Unit 7 by adding backpropagation, gradient descent visualisation, and ANN error analysis.  
<a href="https://github.com/your-username/ml-eportfolio/blob/main/unit8_ann_training.ipynb" target="_blank">View on GitHub</a>
</p>

<h4>ü™û Reflection (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)</h4>
<p>
<strong>WHAT:</strong> Implemented learning through loss minimisation in neural networks.<br />
<strong>SO WHAT:</strong> Backpropagation revealed how sensitive networks are to hyperparameters.<br />
<strong>NOW WHAT:</strong> I plan to experiment with learning rate schedules and different optimisers like Adam and RMSprop.
</p>

<h4>ü§ù Collaboration</h4>
<p>
We compared different loss functions and learning rates. I helped visualise error convergence across epochs for the group.
</p>

<h4>üì∏ Evidence</h4>
<p>
Training logs, loss curves, and annotated backpropagation code in appendix.
</p>

<h4>üõ†Ô∏è Skills & Competencies</h4>
<ul>
  <li>Applied backpropagation to train neural networks</li>
  <li>Analysed loss and gradient descent over time</li>
</ul>

<hr />


<h2>Unit 9 ‚Äì Introduction to Convolutional Neural Networks</h2>

<h4>üß† Summary of Learning</h4>
<p>
This unit introduced CNNs, their role in visual recognition, and the architecture of convolutional, pooling, and fully connected layers. We also briefly explored GANs and transformers as emerging deep learning architectures.
</p>

<h4>üìÅ Artefact</h4>
<p>
Built and tested a basic CNN for digit recognition using a sample image dataset. Modified prediction inputs to observe accuracy.  
<a href="https://github.com/your-username/ml-eportfolio/blob/main/unit9_cnn_object_recognition.ipynb" target="_blank">View on GitHub</a>
</p>

<h4>ü™û Reflection (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)</h4>
<p>
<strong>WHAT:</strong> I trained a CNN using Keras and visualised prediction outputs.<br />
<strong>SO WHAT:</strong> CNNs significantly outperformed dense networks on image tasks.<br />
<strong>NOW WHAT:</strong> I plan to explore transfer learning using pre-trained CNNs like VGG16 or ResNet.
</p>

<h4>ü§ù Collaboration</h4>
<p>
Shared model predictions and discussed accuracy drop for misclassified digits. Peer validation of CNN architecture helped refine final layer choices.
</p>

<h4>üì∏ Evidence</h4>
<p>
Model accuracy logs, prediction visual, and misclassification samples included in appendix.
</p>

<h4>üõ†Ô∏è Skills & Competencies</h4>
<ul>
  <li>Implemented convolutional layers and max pooling in Keras</li>
  <li>Diagnosed misclassifications through visual error analysis</li>
</ul>

<hr />


<h2>Unit 10 ‚Äì Natural Language Processing (NLP)</h2>

<h4>üß† Summary of Learning</h4>
<p>
This unit introduced transformer-based NLP models (BERT, GPT, T5) and their role in text processing, summarisation, and generation. We explored evaluation metrics such as BLEU and ROUGE, and discussed the influence of self-supervised learning in scaling NLP applications.
</p>

<h4>üìÅ Artefact</h4>
<p>
Fine-tuned a pretrained BERT model for sentiment classification. Evaluated performance using F1-score and confusion matrix.  
<a href="https://github.com/your-username/ml-eportfolio/blob/main/unit10_nlp_transformers.ipynb" target="_blank">View on GitHub</a>
</p>

<h4>ü™û Reflection (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)</h4>
<p>
<strong>WHAT:</strong> I applied transfer learning in NLP using HuggingFace‚Äôs `transformers` library.<br />
<strong>SO WHAT:</strong> This showed the impact of pretraining and contextual embeddings.<br />
<strong>NOW WHAT:</strong> I aim to evaluate larger models like T5 and experiment with summarisation and Q&A pipelines.
</p>

<h4>ü§ù Collaboration</h4>
<p>
Collaborated on fine-tuning datasets and tokenizer choices. Exchanged feedback on performance metrics and label imbalance.
</p>

<h4>üì∏ Evidence</h4>
<p>
Confusion matrix, tokenizer output, and training log screenshots available in appendix.
</p>

<h4>üõ†Ô∏è Skills & Competencies</h4>
<ul>
  <li>Applied BERT for sentiment classification</li>
  <li>Evaluated NLP models using precision, recall, and BLEU score</li>
</ul>

<hr />


<h2>Unit 11 ‚Äì Model Selection and Evaluation</h2>

<h4>üß† Summary of Learning</h4>
<p>
This unit covered hyperparameter tuning, cross-validation, and evaluation metrics beyond accuracy (e.g., AUC, F1, Precision/Recall). It introduced MLOps principles such as reproducibility, model deployment, and performance monitoring in production.
</p>

<h4>üìÅ Artefact</h4>
<p>
Ran `model_Performance_Measurement.ipynb` and compared AUC and R¬≤ across multiple model types and parameter settings.  
<a href="https://github.com/your-username/ml-eportfolio/blob/main/unit11_model_selection.ipynb" target="_blank">View on GitHub</a>
</p>

<h4>ü™û Reflection (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)</h4>
<p>
<strong>WHAT:</strong> I tuned hyperparameters and monitored impact on key evaluation metrics.<br />
<strong>SO WHAT:</strong> This helped me see how choice of metric changes model evaluation focus.<br />
<strong>NOW WHAT:</strong> I aim to build automated evaluation pipelines as part of an MLOps workflow.
</p>

<h4>ü§ù Collaboration</h4>
<p>
Led a GitHub issue discussion around cross-validation split strategy. Peer-reviewed ROC curve plots and helped clarify misinterpretations.
</p>

<h4>üì∏ Evidence</h4>
<p>
Evaluation score tables, AUC-ROC curve plots, and parameter impact charts added in appendix.
</p>

<h4>üõ†Ô∏è Skills & Competencies</h4>
<ul>
  <li>Applied hyperparameter tuning and performance monitoring</li>
  <li>Explored MLOps principles and metric-based model evaluation</li>
</ul>

<hr />


<h2>Unit 12 ‚Äì Industry 4.0 and Machine Learning</h2>

<h4>üß† Summary of Learning</h4>
<p>
This final unit examined the convergence of machine learning with Industry 4.0 trends: digital twins, Edge AI, self-supervised learning, and neural architecture search. It also introduced the vision of Industry 5.0‚Äîhuman-centric and sustainable AI.
</p>

<h4>üìÅ Artefact</h4>
<p>
Wrote a wiki post discussing the role of Self-Supervised Learning and Edge AI in autonomous manufacturing systems.  
<a href="https://github.com/your-username/ml-eportfolio/blob/main/unit12_industry4.0_reflection.md" target="_blank">View Wiki Post</a>
</p>

<h4>ü™û Reflection (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)</h4>
<p>
<strong>WHAT:</strong> Explored how ML is driving automation and AI-driven systems across industries.<br />
<strong>SO WHAT:</strong> I now see ML not just as a technical skill but a socio-technical force.<br />
<strong>NOW WHAT:</strong> I aim to explore applications in AI for sustainability, energy efficiency, and smart healthcare.
</p>

<h4>ü§ù Collaboration</h4>
<p>
Engaged in discussions on the ethical use of autonomous AI. Shared readings on bias in facial recognition and contributed to the digital twin case study.
</p>

<h4>üì∏ Evidence</h4>
<p>
Screenshots of wiki contribution and reading annotations included in appendix.
</p>

<h4>üõ†Ô∏è Skills & Competencies</h4>
<ul>
  <li>Connected ML concepts to industry-wide transformations</li>
  <li>Reflected on ethical, legal, and social implications of AI</li>
</ul>

<hr />

      <h4>üìö References</h4>
      <p>[Add APA/Harvard formatted sources used across all units and reflection]</p>
    </div>
  </section>

  <footer>
    <div class="container">
      <p>¬© 2025 Jasim Alzaabi</p>
    </div>
  </footer>
</body>
</html>
