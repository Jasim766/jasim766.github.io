<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Machine Learning | Jasim Alzaabi</title>
  <link rel="stylesheet" href="assets/css/style.css" />
</head>
<body>
  <header class="hero smaller">
    <div class="container">
      <h1>Machine Learning E-Portfolio</h1>
      <nav class="navbar">
        <a href="index.html">Home</a>
        <a href="about.html">About</a>
        <a href="machine-learning.html" class="active">Machine Learning</a>
        <a href="research-methods.html">Research Methods</a>
      </nav>
    </div>
  </header>

  <section class="section">
    <div class="container">
     <h2>Unit 1 â€“ Introduction to Machine Learning</h2>

<h4>Summary of Learning</h4>
<p>
In this opening unit, we explored the fundamental concepts of machine learning (ML) and its growing role in shaping modern industries and everyday life. The unit provided an overview of how ML systems learn from data, the relationship between machine learning and big data, and how algorithmic decision-making is transforming both public and private sectors. The lecturecast and reading materials highlighted the evolution of ML technologies and examined the skills required to work effectively in this field.
</p>
<p>
We also discussed the importance of real-time data availability in improving the performance of ML models. Case studies and examples illustrated how algorithms are already embedded in tasks such as personalisation, automation, and risk profiling. In parallel, we reflected on some of the risks and limitations of using machine learning at scale, such as data bias, transparency challenges, and ethical concerns.
</p>

<h4>Artefact</h4>
<p>
I contributed to the discussion forum with an initial post examining the Facebookâ€“Cambridge Analytica scandal as a real-world example of the ethical risks in data-driven profiling. The post outlined the legal and reputational consequences faced by the organisations involved and raised questions about the accountability of ML systems in public communication and politics.
</p>

<h4>Reflection (WHAT â†’ SO WHAT â†’ NOW WHAT)</h4>
<p>
<strong>What:</strong> This week introduced me to the broad scope of machine learning and its influence across different industries. It was my first opportunity to link theoretical learning with a real-world application by critically analysing a known data misuse case.<br><br>

<strong>So What:</strong> The discussion helped me realise that ML is not only a technical discipline but also one that intersects deeply with ethics, law, and public trust. Before this, I tended to think of algorithms mainly as optimisation tools. Now I understand they also carry social and political consequences.<br><br>

<strong>Now What:</strong> Going forward, I intend to incorporate ethical evaluation into all stages of my machine learning projects. This includes paying closer attention to data sourcing, fairness in predictions, and transparency of models, particularly when used in sensitive contexts like healthcare or finance.
</p>

<h4>Collaboration</h4>
<p>
As part of our initial team activity, I contributed to the drafting of the group contract by proposing communication protocols and suggesting weekly task updates via Slack. We also agreed on using shared cloud documents to streamline team coordination. These early decisions laid a strong foundation for our group work and helped clarify expectations.
</p>


<h4>Skills & Competencies</h4>
<ul>
  <li>Developed a foundational understanding of supervised and unsupervised learning</li>
  <li>Reflected on the ethical implications of data-driven decision-making</li>
  <li>Practised team collaboration and contributed to group project planning</li>
</ul>

<h4>References</h4>
<ul>
  <li>Schwab, K. and Zahid, A. (2020). <em>The Future of Jobs Report 2020</em>. World Economic Forum.</li>
  <li>World Economic Forum (2025). <em>The Future of Jobs Report 2025</em>.</li>
  <li>BBC News (2021). <em>The Great Hack. Facebook sued over Cambridge Analytica data scandal</em>.</li>
</ul>

<hr />


<h2>Unit 2 â€“ Exploratory Data Analysis</h2>

<h4>ğŸ§  Summary of Learning</h4>
<p>
This unit focused on the foundational process of Exploratory Data Analysis (EDA), a critical step in machine learning pipelines. We explored key EDA steps such as data validation, anomaly detection, statistical profiling, and visual pattern recognition. Emphasis was placed on understanding how EDA contributes to effective feature selection and iterative feature engineering.
</p>

<h4>ğŸ“ Artefact</h4>
<p>
For this unit, I created a Jupyter Notebook where I performed EDA on a pre-cleaned customer churn dataset. I applied descriptive statistics, boxplots, and heatmaps to detect multicollinearity and outliers.  
<a href="https://github.com/your-username/ml-eportfolio/blob/main/unit2_eda.ipynb" target="_blank">View on GitHub</a>
</p>

<h4>ğŸª Reflection (WHAT â†’ SO WHAT â†’ NOW WHAT)</h4>
<p>
<strong>WHAT:</strong> I explored the practical process of EDA using Python libraries like Pandas, Matplotlib, and Seaborn.<br />
<strong>SO WHAT:</strong> I saw how anomalies and data distribution can significantly affect the learning phase and performance of a model.<br />
<strong>NOW WHAT:</strong> Iâ€™ll make EDA a standard step in all future ML pipelines, documenting findings for reproducibility and feature engineering refinement.
</p>

<h4>ğŸ¤ Collaboration</h4>
<p>
Our team aligned on EDA practices during a group call. I shared my notebook and proposed dropping several highly correlated variables, which was adopted by others.
</p>

<h4>ğŸ“¸ Evidence</h4>
<p>
Screenshots of boxplots, correlation matrices, and data distribution charts included in appendix. Notebook hosted on GitHub repo.
</p>

<h4>ğŸ› ï¸ Skills & Competencies</h4>
<ul>
  <li>Applied statistical and visual techniques for dataset exploration</li>
  <li>Identified feature selection criteria and data integrity risks</li>
</ul>

<hr />


<h2>Unit 3 â€“ Correlation and Regression</h2>

<h4>ğŸ§  Summary of Learning</h4>
<p>
This unit explored the use of correlation to measure variable relationships, and regression to express those relationships mathematically for prediction. We studied Pearson and Spearman correlation coefficients, and implemented both simple and multiple linear regression models using Python.
</p>

<h4>ğŸ“ Artefact</h4>
<p>
Jupyter Notebook demonstrating Pearson, Spearman, simple linear regression, and multiple regression using a housing dataset.  
<a href="https://github.com/your-username/ml-eportfolio/blob/main/unit3_correlation_regression.ipynb" target="_blank">View on GitHub</a>
</p>

<h4>ğŸª Reflection (WHAT â†’ SO WHAT â†’ NOW WHAT)</h4>
<p>
<strong>WHAT:</strong> I ran statistical tests and built regression models on numeric datasets.<br />
<strong>SO WHAT:</strong> This helped me grasp how correlation analysis feeds directly into regression and feature selection.<br />
<strong>NOW WHAT:</strong> Iâ€™ll incorporate correlation matrices in my future EDA workflows and use regression models to establish performance baselines before advanced ML techniques.
</p>

<h4>ğŸ¤ Collaboration</h4>
<p>
Shared regression visuals and coefficient analysis with my team. Reviewed and provided feedback on another teammateâ€™s interpretation of multicollinearity.
</p>

<h4>ğŸ“¸ Evidence</h4>
<p>
Notebook outputs include regression lines, correlation heatmaps, and residual plots. Notebook and figures stored in GitHub repository.
</p>

<h4>ğŸ› ï¸ Skills & Competencies</h4>
<ul>
  <li>Built linear models using NumPy, Pandas, and Seaborn</li>
  <li>Interpreted RÂ², p-values, and correlation strength</li>
</ul>

<hr />


<h2>Unit 4 â€“ Linear Regression with Scikit-Learn</h2>

<h4>ğŸ§  Summary of Learning</h4>
<p>
This unit introduced the use of Scikit-Learn for implementing both simple and multiple linear regression models. Emphasis was placed on preparing input features, training regression models, and evaluating them using RÂ², MAE, and RMSE. We also examined Scikit-Learnâ€™s modular design and how it fits into an end-to-end ML workflow.
</p>

<h4>ğŸ“ Artefact</h4>
<p>
Jupyter Notebook on regression modeling with Scikit-Learn using a car price dataset.  
<a href="https://github.com/your-username/ml-eportfolio/blob/main/unit4_linear_regression_scikit.ipynb" target="_blank">View on GitHub</a>
</p>

<h4>ğŸª Reflection (WHAT â†’ SO WHAT â†’ NOW WHAT)</h4>
<p>
<strong>WHAT:</strong> I used Scikit-Learn to fit and evaluate linear regression models.<br />
<strong>SO WHAT:</strong> I gained practical knowledge of pipeline construction and model validation.<br />
<strong>NOW WHAT:</strong> I will consistently use `train_test_split`, `LinearRegression`, and validation metrics when developing predictive models in real-world scenarios.
</p>

<h4>ğŸ¤ Collaboration</h4>
<p>
Led a short tutorial on Scikit-Learnâ€™s regression module during our weekly team call. I also helped standardize our teamâ€™s approach to evaluating linear models.
</p>

<h4>ğŸ“¸ Evidence</h4>
<p>
Model outputs, RÂ² comparisons, and residual plots documented in GitHub notebook. Code and visual results also shared in team repo.
</p>

<h4>ğŸ› ï¸ Skills & Competencies</h4>
<ul>
  <li>Used Scikit-Learn to implement linear regression models</li>
  <li>Applied model evaluation techniques (RÂ², MAE, RMSE)</li>
</ul>

<hr />


<h2>Unit 5 â€“ Clustering</h2>

<h4>ğŸ§  Summary of Learning</h4>
<p>
This unit explored unsupervised learning, focusing on clustering techniques such as k-means and agglomerative clustering. We studied how clustering groups data points based on similarity measures like Euclidean distance and evaluated clustering quality using metrics like SSE and silhouette score. The broader application of clustering in pattern recognition, image analysis, and business intelligence was also covered.
</p>

<h4>ğŸ“ Artefact</h4>
<p>
I completed two visual clustering animations (uniform and custom point sets), then reflected on cluster formation logic in the module wiki. I also created a clustering notebook demonstrating k-means clustering using Scikit-Learn.  
<a href="https://github.com/your-username/ml-eportfolio/blob/main/unit5_clustering.ipynb" target="_blank">View on GitHub</a>
</p>

<h4>ğŸª Reflection (WHAT â†’ SO WHAT â†’ NOW WHAT)</h4>
<p>
<strong>WHAT:</strong> I applied unsupervised clustering algorithms and evaluated clusters using silhouette scores.<br />
<strong>SO WHAT:</strong> This gave me insight into model-free learning methods, especially the challenge of defining 'optimal' clusters.<br />
<strong>NOW WHAT:</strong> Iâ€™ll explore DBSCAN and hierarchical clustering next and use silhouette analysis for cluster evaluation across future datasets.
</p>

<h4>ğŸ¤ Collaboration</h4>
<p>
I commented on two peersâ€™ wiki posts, offering alternate interpretations of cluster boundaries. I also shared my notebook on k-means, which a teammate extended with new data features.
</p>

<h4>ğŸ“¸ Evidence</h4>
<p>
Screenshots of animation choices, silhouette graphs, and final clusters are included in appendix. Jupyter Notebook and wiki post are both referenced.
</p>

<h4>ğŸ› ï¸ Skills & Competencies</h4>
<ul>
  <li>Applied k-means clustering and evaluated with silhouette scoring</li>
  <li>Compared visual vs. algorithmic cluster assessment strategies</li>
</ul>

<hr />

<h2>Unit 6 â€“ Clustering with Python</h2>

<h4>ğŸ§  Summary of Learning</h4>
<p>
This unit applied the K-Means clustering algorithm using Scikit-Learn on real-life datasets. We explored the use of clustering to reveal patterns in unlabelled data and interpreted clusters through visualisation and inertia/silhouette metrics.
</p>

<h4>ğŸ“ Artefact</h4>
<p>
Created a Jupyter Notebook clustering customer data using K-Means. Included elbow plots and cluster interpretation.  
<a href="https://github.com/your-username/ml-eportfolio/blob/main/unit6_kmeans.ipynb" target="_blank">View on GitHub</a>
</p>

<h4>ğŸª Reflection (WHAT â†’ SO WHAT â†’ NOW WHAT)</h4>
<p>
<strong>WHAT:</strong> I applied clustering on retail data and evaluated the optimal number of clusters.<br />
<strong>SO WHAT:</strong> It helped me see how unsupervised methods can expose structure and user behaviour.<br />
<strong>NOW WHAT:</strong> I aim to explore clustering evaluation in more depth, comparing DBSCAN and GMM models.
</p>

<h4>ğŸ¤ Collaboration</h4>
<p>
We discussed differences in cluster interpretation. I presented the elbow method and led our visual validation.
</p>

<h4>ğŸ“¸ Evidence</h4>
<p>
Elbow chart, cluster scatter plot and silhouette score visual saved in appendix and GitHub.
</p>

<h4>ğŸ› ï¸ Skills & Competencies</h4>
<ul>
  <li>Used K-Means for customer segmentation</li>
  <li>Evaluated cluster quality using silhouette score</li>
</ul>

<hr />


<h2>Unit 7 â€“ Introduction to Artificial Neural Networks</h2>

<h4>ğŸ§  Summary of Learning</h4>
<p>
We explored the structure and function of ANNs, including perceptrons and activation functions. This unit covered biological inspiration, multilayer structures, and their applications across various domains.
</p>

<h4>ğŸ“ Artefact</h4>
<p>
Built a simple perceptron, AND gate simulation, and a multi-layer perceptron with sigmoid activation using Python.  
<a href="https://github.com/your-username/ml-eportfolio/blob/main/unit7_ann_intro.ipynb" target="_blank">View on GitHub</a>
</p>

<h4>ğŸª Reflection (WHAT â†’ SO WHAT â†’ NOW WHAT)</h4>
<p>
<strong>WHAT:</strong> I simulated neural networks from scratch using NumPy.<br />
<strong>SO WHAT:</strong> I grasped the role of non-linear activations in enabling expressive learning.<br />
<strong>NOW WHAT:</strong> Iâ€™ll transition to using frameworks like TensorFlow and PyTorch for more scalable ANN implementations.
</p>

<h4>ğŸ¤ Collaboration</h4>
<p>
Shared logic gate results and discussed pros/cons of sigmoid vs ReLU. Team peer-reviewed activation function performance.
</p>

<h4>ğŸ“¸ Evidence</h4>
<p>
Logic gate screenshots, weight updates, and loss graphs added to appendix.
</p>

<h4>ğŸ› ï¸ Skills & Competencies</h4>
<ul>
  <li>Simulated basic ANN models from scratch</li>
  <li>Understood forward propagation and activations</li>
</ul>

<hr />


<h2>Unit 8 â€“ Training an Artificial Neural Network</h2>

<h4>ğŸ§  Summary of Learning</h4>
<p>
We explored training neural networks through backpropagation and weight updates. Key topics included error propagation, gradient descent, and adjusting weights through optimisation.
</p>

<h4>ğŸ“ Artefact</h4>
<p>
Extended the MLP implementation from Unit 7 by adding backpropagation, gradient descent visualisation, and ANN error analysis.  
<a href="https://github.com/your-username/ml-eportfolio/blob/main/unit8_ann_training.ipynb" target="_blank">View on GitHub</a>
</p>

<h4>ğŸª Reflection (WHAT â†’ SO WHAT â†’ NOW WHAT)</h4>
<p>
<strong>WHAT:</strong> Implemented learning through loss minimisation in neural networks.<br />
<strong>SO WHAT:</strong> Backpropagation revealed how sensitive networks are to hyperparameters.<br />
<strong>NOW WHAT:</strong> I plan to experiment with learning rate schedules and different optimisers like Adam and RMSprop.
</p>

<h4>ğŸ¤ Collaboration</h4>
<p>
We compared different loss functions and learning rates. I helped visualise error convergence across epochs for the group.
</p>

<h4>ğŸ“¸ Evidence</h4>
<p>
Training logs, loss curves, and annotated backpropagation code in appendix.
</p>

<h4>ğŸ› ï¸ Skills & Competencies</h4>
<ul>
  <li>Applied backpropagation to train neural networks</li>
  <li>Analysed loss and gradient descent over time</li>
</ul>

<hr />


<h2>Unit 9 â€“ Introduction to Convolutional Neural Networks</h2>

<h4>ğŸ§  Summary of Learning</h4>
<p>
This unit introduced CNNs, their role in visual recognition, and the architecture of convolutional, pooling, and fully connected layers. We also briefly explored GANs and transformers as emerging deep learning architectures.
</p>

<h4>ğŸ“ Artefact</h4>
<p>
Built and tested a basic CNN for digit recognition using a sample image dataset. Modified prediction inputs to observe accuracy.  
<a href="https://github.com/your-username/ml-eportfolio/blob/main/unit9_cnn_object_recognition.ipynb" target="_blank">View on GitHub</a>
</p>

<h4>ğŸª Reflection (WHAT â†’ SO WHAT â†’ NOW WHAT)</h4>
<p>
<strong>WHAT:</strong> I trained a CNN using Keras and visualised prediction outputs.<br />
<strong>SO WHAT:</strong> CNNs significantly outperformed dense networks on image tasks.<br />
<strong>NOW WHAT:</strong> I plan to explore transfer learning using pre-trained CNNs like VGG16 or ResNet.
</p>

<h4>ğŸ¤ Collaboration</h4>
<p>
Shared model predictions and discussed accuracy drop for misclassified digits. Peer validation of CNN architecture helped refine final layer choices.
</p>

<h4>ğŸ“¸ Evidence</h4>
<p>
Model accuracy logs, prediction visual, and misclassification samples included in appendix.
</p>

<h4>ğŸ› ï¸ Skills & Competencies</h4>
<ul>
  <li>Implemented convolutional layers and max pooling in Keras</li>
  <li>Diagnosed misclassifications through visual error analysis</li>
</ul>

<hr />


<h2>Unit 10 â€“ Natural Language Processing (NLP)</h2>

<h4>ğŸ§  Summary of Learning</h4>
<p>
This unit introduced transformer-based NLP models (BERT, GPT, T5) and their role in text processing, summarisation, and generation. We explored evaluation metrics such as BLEU and ROUGE, and discussed the influence of self-supervised learning in scaling NLP applications.
</p>

<h4>ğŸ“ Artefact</h4>
<p>
Fine-tuned a pretrained BERT model for sentiment classification. Evaluated performance using F1-score and confusion matrix.  
<a href="https://github.com/your-username/ml-eportfolio/blob/main/unit10_nlp_transformers.ipynb" target="_blank">View on GitHub</a>
</p>

<h4>ğŸª Reflection (WHAT â†’ SO WHAT â†’ NOW WHAT)</h4>
<p>
<strong>WHAT:</strong> I applied transfer learning in NLP using HuggingFaceâ€™s `transformers` library.<br />
<strong>SO WHAT:</strong> This showed the impact of pretraining and contextual embeddings.<br />
<strong>NOW WHAT:</strong> I aim to evaluate larger models like T5 and experiment with summarisation and Q&A pipelines.
</p>

<h4>ğŸ¤ Collaboration</h4>
<p>
Collaborated on fine-tuning datasets and tokenizer choices. Exchanged feedback on performance metrics and label imbalance.
</p>

<h4>ğŸ“¸ Evidence</h4>
<p>
Confusion matrix, tokenizer output, and training log screenshots available in appendix.
</p>

<h4>ğŸ› ï¸ Skills & Competencies</h4>
<ul>
  <li>Applied BERT for sentiment classification</li>
  <li>Evaluated NLP models using precision, recall, and BLEU score</li>
</ul>

<hr />


<h2>Unit 11 â€“ Model Selection and Evaluation</h2>

<h4>ğŸ§  Summary of Learning</h4>
<p>
This unit covered hyperparameter tuning, cross-validation, and evaluation metrics beyond accuracy (e.g., AUC, F1, Precision/Recall). It introduced MLOps principles such as reproducibility, model deployment, and performance monitoring in production.
</p>

<h4>ğŸ“ Artefact</h4>
<p>
Ran `model_Performance_Measurement.ipynb` and compared AUC and RÂ² across multiple model types and parameter settings.  
<a href="https://github.com/your-username/ml-eportfolio/blob/main/unit11_model_selection.ipynb" target="_blank">View on GitHub</a>
</p>

<h4>ğŸª Reflection (WHAT â†’ SO WHAT â†’ NOW WHAT)</h4>
<p>
<strong>WHAT:</strong> I tuned hyperparameters and monitored impact on key evaluation metrics.<br />
<strong>SO WHAT:</strong> This helped me see how choice of metric changes model evaluation focus.<br />
<strong>NOW WHAT:</strong> I aim to build automated evaluation pipelines as part of an MLOps workflow.
</p>

<h4>ğŸ¤ Collaboration</h4>
<p>
Led a GitHub issue discussion around cross-validation split strategy. Peer-reviewed ROC curve plots and helped clarify misinterpretations.
</p>

<h4>ğŸ“¸ Evidence</h4>
<p>
Evaluation score tables, AUC-ROC curve plots, and parameter impact charts added in appendix.
</p>

<h4>ğŸ› ï¸ Skills & Competencies</h4>
<ul>
  <li>Applied hyperparameter tuning and performance monitoring</li>
  <li>Explored MLOps principles and metric-based model evaluation</li>
</ul>

<hr />


<h2>Unit 12 â€“ Industry 4.0 and Machine Learning</h2>

<h4>ğŸ§  Summary of Learning</h4>
<p>
This final unit examined the convergence of machine learning with Industry 4.0 trends: digital twins, Edge AI, self-supervised learning, and neural architecture search. It also introduced the vision of Industry 5.0â€”human-centric and sustainable AI.
</p>

<h4>ğŸ“ Artefact</h4>
<p>
Wrote a wiki post discussing the role of Self-Supervised Learning and Edge AI in autonomous manufacturing systems.  
<a href="https://github.com/your-username/ml-eportfolio/blob/main/unit12_industry4.0_reflection.md" target="_blank">View Wiki Post</a>
</p>

<h4>ğŸª Reflection (WHAT â†’ SO WHAT â†’ NOW WHAT)</h4>
<p>
<strong>WHAT:</strong> Explored how ML is driving automation and AI-driven systems across industries.<br />
<strong>SO WHAT:</strong> I now see ML not just as a technical skill but a socio-technical force.<br />
<strong>NOW WHAT:</strong> I aim to explore applications in AI for sustainability, energy efficiency, and smart healthcare.
</p>

<h4>ğŸ¤ Collaboration</h4>
<p>
Engaged in discussions on the ethical use of autonomous AI. Shared readings on bias in facial recognition and contributed to the digital twin case study.
</p>

<h4>ğŸ“¸ Evidence</h4>
<p>
Screenshots of wiki contribution and reading annotations included in appendix.
</p>

<h4>ğŸ› ï¸ Skills & Competencies</h4>
<ul>
  <li>Connected ML concepts to industry-wide transformations</li>
  <li>Reflected on ethical, legal, and social implications of AI</li>
</ul>

<hr />

      <h4>ğŸ“š References</h4>
      <p>[Add APA/Harvard formatted sources used across all units and reflection]</p>
    </div>
  </section>

  <footer>
    <div class="container">
      <p>Â© 2025 Jasim Alzaabi</p>
    </div>
  </footer>
</body>
</html>
