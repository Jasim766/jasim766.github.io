<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Machine Learning | Jasim Alzaabi</title>
  <link rel="stylesheet" href="assets/css/style.css" />
</head>
<body>
  <header class="hero smaller">
    <div class="container">
      <h1>Machine Learning E-Portfolio</h1>
      <nav class="navbar">
        <a href="index.html">Home</a>
        <a href="about.html">About</a>
        <a href="machine-learning.html" class="active">Machine Learning</a>
        <a href="research-methods.html">Research Methods</a>
      </nav>
    </div>
  </header>

  <section class="section">
    <div class="container">
     <h2>Unit 1 ‚Äì Introduction to Machine Learning</h2>

<h4> Summary of Learning</h4>
<p>
This first unit provided an introduction to the core principles of machine learning (ML), its relationship with artificial intelligence, and the wider implications of deploying such systems at scale. We discussed how ML is integrated into everyday tools and services, from recommendation engines to automated decision-making. The session also introduced the distinction between Industry 4.0 and Industry 5.0‚Äîhighlighting a shift from automation-focused systems to more human-centric, ethically guided technologies (Metcalf, 2024).
</p>
<p>
A key takeaway from this week was the growing concern around how rapidly implemented ML systems can create unintended risks‚Äîparticularly when deployed without sufficient consideration for privacy, security, and user impact. As technologies become more integrated into industry and society, their design must reflect not only performance metrics, but also values such as fairness, transparency, and resilience.
</p>

<h4>üìÅ Artefact</h4>
<p>
For this unit, I contributed to the discussion forum with a post titled <em>‚ÄúM&S Cyberattack in UK Retail‚Äù</em>. In this post, I analysed the April 2025 cyberattack on Marks & Spencer (M&S) as a case study that illustrates how a narrow focus on speed and automation‚Äîas typical of Industry 4.0‚Äîcan expose serious security and ethical vulnerabilities. I drew on Metcalf‚Äôs (2024) concept of Industry 5.0 to propose how a more adaptive, human-in-the-loop approach could have reduced the scale of disruption and protected consumer trust.  
<a href="https://github.com/your-username/ml-eportfolio/blob/main/unit1_ms_cyberattack_reflection.md" target="_blank">View Discussion Post</a>
</p>

<h4>ü™û Reflection (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)</h4>
<p>
<strong>What:</strong> The unit introduced machine learning as a key driver in both innovation and risk within modern industry. Analysing the M&S incident helped me understand the real-world consequences when ethical design is overlooked in favour of efficiency or automation.<br><br>

<strong>So What:</strong> This case changed my perception of ML systems‚Äîfrom seeing them as purely technical achievements to understanding them as socio-technical systems. The scale of customer impact and reputational loss at M&S highlighted how quickly trust can break down if systems fail to account for human oversight and resilience (Ravikumar & Young, 2025; Gallagher, 2025).<br><br>

<strong>Now What:</strong> Moving forward, I plan to critically evaluate ML architectures not only for performance, but also for ethical integrity, data protection, and risk mitigation. I also want to explore how principles like redundancy and decentralisation could be designed into future systems, in line with Industry 5.0 values.
</p>

<h4> Collaboration</h4>
<p>
In our team formation session, I helped shape our initial group contract by proposing weekly communication via whatsapp and google meet . We agreed to use shared Google Docs for updates and keep regular check-ins.
</p>

<h4> Evidence</h4>
<p>
My forum post, titled ‚ÄúM&S Cyberattack in UK Retail‚Äù, 
  M&S Cyberattack  in UK Retail

 Metcalf talks about how Industry 5.0 is not just about adding more tech or making everything automatic like in Industry 4.0, but more about making systems that think about people, are flexible, and follow ethical ideas(Metcalf, 2024). This is important in areas like retail, where digital changes happened super-fast, but in many cases, they didn‚Äôt really think about how to protect workers or customers properly. A lot of systems were built just to be fast or efficient, not to be fair or safe for everyone involved(Metcalf, 2024).

 One serious case that shows how scary  these systems can be happened in April 2025, when Marks & Spencer (M&S), one of the biggest retail companies in the UK, was hit by a major cyberattack. The attackers got into customer data though payment info wasn‚Äôt stolen and caused big problems with online shopping and even systems inside physical stores (Ravikumar &  Young, 2025). Experts said the company was losing around ¬£25 to ¬£30 million every week, especially in its clothing and home departments. On top of that, M&S‚Äôs share price dropped sharply by about 15% which meant they lost around ¬£1.3 billion in market value (Gallagher, 2025).

 What happened at M&S shows the downside of how Industry 4.0 has been applied‚Äîfocusing too much on speed, automation, and storing everything in one place, without really thinking about what happens if things go wrong (Heuser,  & B., & Wang, L., 2021). Metcalf‚Äôs view of Industry 5.0 goes in a different direction. It‚Äôs more about making systems that can adapt, involve human decisions, and are built to be strong against unexpected problems. If M&S had already been using a setup where AI systems worked with human teams, had tools to spot risks early, and didn‚Äôt rely on just one central system, maybe the damage wouldn‚Äôt have been so massive (Metcalf, 2024).

  The attack seriously damaged customer trust in M&S.  their customers shared their frustration and fear on forums and Trustpilot, mainly about personal data being exposed. This supports research showing that loyalty depends heavily on data security and ethical tech use (Mittelstadt, & Floridi,, 2017)
  
</p>

<h4>üõ†Ô∏è Skills & Competencies</h4>
<ul>
  <li>Developed a foundational understanding of machine learning applications and challenges</li>
  <li>Critically analysed real-world incident using Industry 5.0 theory</li>
  <li>Contributed to team collaboration and communication strategy</li>
</ul>

<h4>References</h4>
<ul>
  <li>Metcalf, G.S. (2024) <em>An Introduction to Industry 5.0: History, Foundations, and Futures</em>. SpringerLink.</li>
  <li>Ravikumar, S. and Young, S. (2025) <em>UK‚Äôs M&S says customer data was taken in cyber attack</em>. Reuters.</li>
  <li>Gallagher, R. (2025) <em>M&S Cybersecurity Incident Still Causing Disruption</em>. Bloomberg.</li>
  <li>Mittelstadt, B. and Floridi, L. (2017) <em>Why a Right to Explanation of Automated Decision-Making Does Not Exist in GDPR</em>. International Data Privacy Law.</li>
</ul>

<hr />



<h2>Unit 2 ‚Äì Exploratory Data Analysis</h2>

<h4>Summary of Learning</h4>
<p>
This unit introduced the foundational principles of Exploratory Data Analysis (EDA), focusing on the importance of understanding data structure, identifying anomalies, and preparing datasets for machine learning. We reviewed key concepts such as feature distribution, outlier detection, and correlation analysis, and discussed how EDA helps form the basis for accurate and reliable modelling.
</p>
<p>
One of the core insights from this unit was that raw data is rarely ready for immediate use in machine learning. EDA enables practitioners to clean, visualise, and assess data before moving to algorithm development. This step is essential for both supervised and unsupervised learning, ensuring the quality of inputs and improving downstream model performance.
</p>

<h4>Artefact</h4>
<p>
my artefact is a reflective summary and personal notes taken while engaging with the core reading (Bishop & Bishop, 2024, Chapter 3) and lecturecast. I participated in the forum and group chat discussions on EDA techniques, and shared thoughts on how data anomalies can mislead model interpretation if not properly identified. My reflections focused on real-world risks of neglecting EDA in business-critical applications, such as financial forecasting or fraud detection.
</p>

<h4>Reflection (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)</h4>
<p>
<strong>What:</strong> I learned that EDA is not simply about generating plots or statistics, but about exploring data with intent. It involves questioning the reliability, relevance, and completeness of the dataset before doing any modelling work.<br><br>

<strong>So What:</strong> This changed my perception of ML workflows. I used to think modelling came first, but now I understand how poor EDA leads to overfitting, biased models, or misleading insights. The reading and peer comments reinforced how common data quality issues are, and how easily they are overlooked without structured analysis.<br><br>

<strong>Now What:</strong> For future assignments, I will build in more time for EDA as a formal process. I also plan to use tools like correlation matrices, summary statistics, and visualisation libraries when I begin working on modelling tasks, even if not required, because I now see their long-term value.
</p>

<h4>Collaboration</h4>
<p>
During the week, I contributed to our team discussion around the importance of data validation in machine learning. I shared insights from the reading and lecturecast, particularly around the concept of "garbage in, garbage out," and how this applies to training models on poorly understood datasets. I also reflected on how EDA can reduce risk in practical applications like medical imaging or credit scoring.
</p>



<h4>Skills & Competencies</h4>
<ul>
  <li>Strengthened understanding of data integrity and its role in ML performance</li>
  <li>Engaged with peer learning through reflective discussion</li>
  <li>Linked theory to practical risks in real-world machine learning deployments</li>
</ul>

<h4>References</h4>
<ul>
  <li>Bishop, C. and Bishop, H. (2024). <em>Deep Learning: Foundations and Concepts</em>. Cambridge University Press. Chapter 3.</li>
  <li>Chernozhukov, V., Hansen, C., & Spindler, M. (2024). <em>Applied Causal Inference Powered by ML and AI</em>. Oxford University Press.</li>
  <li>Patil, P. (2018). <em>What is Exploratory Data Analysis?</em></li>
  <li>Harmadi, A. (2021). <em>10 Things to Do When Conducting Your Exploratory Data Analysis (EDA)</em>.</li>
</ul>

<hr />



<h2>Unit 3 ‚Äì Correlation and Regression</h2>

<h4>Summary of Learning</h4>
<p>
This week‚Äôs content introduced the statistical foundations of correlation and regression. Correlation was examined as a means of quantifying the strength and direction of linear associations between two variables, while regression was explored as a method for constructing predictive relationships. These techniques are essential for supervised learning tasks, especially in understanding how variables interact and how future outcomes can be estimated based on input data.
</p>

<h4>Artefact</h4>
<p>
The unit‚Äôs practical exercises were completed through four Jupyter Notebooks: <code>covariance_pearson_correlation.ipynb</code>, <code>linear_regression.ipynb</code>, <code>multiple_linear_regression.ipynb</code>, and <code>polynomial_regression.ipynb</code>. These notebooks covered the implementation of Pearson correlation, simple linear regression, multivariable linear regression, and polynomial curve fitting. The exercises allowed me to visualise relationships, interpret r-values, and apply regression to real-life scenarios using Python libraries.
</p>

<h4>Reflection (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)</h4>
<p><strong>WHAT:</strong> I explored how correlation identifies statistical association and how regression estimates the nature of this relationship. This was contextualised through hands-on application with generated and real-world data.</p>
<p><strong>SO WHAT:</strong> This experience taught me to approach relationships in data critically. I recognised the importance of distinguishing statistical correlation from causality, and I gained insight into when linear regression may or may not be appropriate for predictive purposes.</p>
<p><strong>NOW WHAT:</strong> I plan to apply these concepts more systematically in future projects. For example, regression will become a tool not only for prediction but also for selecting relevant features and validating model assumptions early in the pipeline.</p>

<h4>Collaboration</h4>
<p>
Collaboration during this unit involved structured peer discussions focused on interpreting regression outputs and identifying common pitfalls. We examined cases of multicollinearity and discussed model assumptions such as normality of residuals and homoscedasticity. Feedback from peers helped reinforce best practices and revealed different approaches to visualising and evaluating regression results.
</p>

<h4>Evidence</h4>
<p>
GitHub repository with notebook submissions:
<ul>
  <li><a href="https://github.com/Jasim766/jasim766.github.io/blob/main/Unit03_Ex1_covariance_pearson_correlation_new.ipynb" target="_blank">Unit03_Ex1_covariance_pearson_correlation_new.ipynb</a></li>
  <li><a href="https://github.com/Jasim766/jasim766.github.io/blob/main/Unit03_Ex2_linear_regression_new.ipynb" target="_blank">Unit03_Ex2_linear_regression_new.ipynb</a></li>
  <li><a href="https://github.com/Jasim766/jasim766.github.io/blob/main/Unit03_Ex3_multiple_linear_regression_new.ipynb" target="_blank">Unit03_Ex3_multiple_linear_regression_new.ipynb</a></li>
  <li><a href="https://github.com/Jasim766/jasim766.github.io/blob/main/Unit03_Ex4_polynomial_regression_new.ipynb" target="_blank">Unit03_Ex4_polynomial_regression_new.ipynb</a></li>
</ul>
These include annotated Python notebooks with correlation heatmaps, regression diagnostics, and R¬≤ evaluations.
</p>


<h4>Skills & Competencies</h4>
<ul>
  <li>Understanding of correlation strength and direction using Pearson‚Äôs r</li>
  <li>Development of linear, multiple, and polynomial regression models</li>
  <li>Hands-on use of NumPy, pandas, scikit-learn, matplotlib, and seaborn for statistical modelling</li>
</ul>

<hr />


<h2>Unit 4 ‚Äì Linear Regression with Scikit-Learn</h2>

<h4>Summary of Learning</h4>
<p>This unit extended our understanding of regression by focusing on Scikit-Learn, a powerful Python library for machine learning. Building on prior statistical foundations, we applied both simple and multiple linear regression models using real-world datasets. The core aim was to learn how to operationalise regression techniques within a machine learning pipeline and critically assess their performance using key metrics like R¬≤. Seminar activities complemented this learning by encouraging both individual coding and collaborative data interpretation.</p>

<h4>Artefact</h4>
<p>The main artefacts included two regression notebooks: one exploring the `FuelConsumption.csv` dataset to understand CO‚ÇÇ emissions, and the other analysing global macroeconomic patterns using `Global_Population.csv` and `Global_GDP.csv`. These activities involved pre-processing, correlation analysis, and predictive modelling using Scikit-Learn. The final outputs included visualisations, regression lines, and interpretation of the Pearson correlation coefficient and regression results.</p>

<h4>Reflection (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)</h4>
<p><strong>WHAT:</strong> I learned how to implement regression models on real datasets and apply statistical interpretation to relationships between population size and economic output.</p>
<p><strong>SO WHAT:</strong> This clarified the importance of data cleaning and preprocessing in modelling. Handling missing data and choosing the right scale impacted both correlation strength and regression accuracy. The exercises showed how flawed data preparation can bias results and obscure insights.</p>
<p><strong>NOW WHAT:</strong> I aim to extend these techniques to more complex models and integrate cross-validation and regularisation to test model robustness. I will also be more mindful of the assumptions behind linear regression and explore nonlinear modelling alternatives where appropriate.</p>

<h4>Collaboration</h4>
<p>During the team seminar, we jointly addressed the Global Population and GDP regression task. I contributed to the data preprocessing strategy and was responsible for writing the project‚Äôs introduction section. Group code reviews helped align our data-cleaning methods and ensured consistent implementation of regression and correlation analysis. Discussing our plots and statistical interpretations together improved the clarity and coherence of our conclusions.</p>

<h4>Evidence</h4>
<p>
GitHub repository with both notebooks: 
<a href="https://github.com/Jasim766/jasim766.github.io/blob/main/Unit04%20demo%20correlation%20regression%20fuel%20consumption.ipynb.ipynb" target="_blank">Fuel Consumption Regression</a>,
<a href="https://github.com/Jasim766/jasim766.github.io/blob/main/Unit04_global_population_gdp_analysis.ipynb" target="_blank">Global Population and GDP Analysis</a>.
Each notebook includes preprocessing steps, correlation plots, regression models, and interpretation of results.
</p>

<h4>Skills & Competencies</h4>
<ul>
  <li>Regression modelling using Scikit-Learn with structured datasets</li>
  <li>Data wrangling, missing value handling, and statistical analysis</li>
  <li>Team collaboration on coding tasks and analytical discussions</li>
  <li>Linking theoretical understanding with applied machine learning tasks</li>
</ul>

<hr />



<h2>Unit 5 ‚Äì Clustering</h2>

<h4>Summary of Learning</h4>
<p>This unit introduced unsupervised learning through clustering, focusing on grouping similar data points without pre-defined labels. Clustering is widely applied in disciplines ranging from bioinformatics to marketing segmentation. We explored key techniques such as K-Means and agglomerative clustering, as well as evaluation metrics to assess cluster quality. We also critically examined the implications of distance metrics and cluster validation techniques such as the silhouette score.</p>

<h4>Artefact</h4>
<p>One practical task involved calculating the <strong>Jaccard Coefficient</strong> using binary categorical data. The dataset featured pathological test results for three individuals: Jack, Mary, and Jim. The objective was to compute similarity scores between each pair to assess their test result overlaps.</p>

<pre><code># Python code to compute Jaccard Coefficient
def jaccard_similarity(set1, set2):
    intersection = len(set1.intersection(set2))
    union = len(set1.union(set2))
    return intersection / union

# Convert test results to sets (excluding Name and Gender)
jack = set(['Y', 'N', 'P', 'N', 'N', 'A'])
mary = set(['Y', 'N', 'P', 'A', 'P', 'N'])
jim = set(['Y', 'P', 'N', 'N', 'N', 'A'])

# Calculate pairwise Jaccard coefficients
jm = jaccard_similarity(jack, mary)
jj = jaccard_similarity(jack, jim)
mj = jaccard_similarity(jim, mary)

print("Jaccard (Jack, Mary):", jm)
print("Jaccard (Jack, Jim):", jj)
print("Jaccard (Jim, Mary):", mj)
</code></pre>

<p>This task demonstrated how binary test data can be transformed for similarity measurement using set theory. Results indicated varying degrees of overlap, reflecting subtle differences in symptoms or diagnoses among the individuals.</p>

<h4>Reflection (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)</h4>
<p><strong>WHAT:</strong> I learned how clustering algorithms identify hidden patterns in unlabelled data and the importance of choosing appropriate distance metrics.</p>
<p><strong>SO WHAT:</strong> This expanded my understanding of unsupervised learning beyond theory, helping me see the importance of evaluation strategies in validating clusters. The Jaccard task gave me a hands-on grasp of binary similarity computations.</p>
<p><strong>NOW WHAT:</strong> I plan to apply clustering to real-world datasets involving customer segmentation and anomaly detection. I will also explore the impact of dimensionality reduction (e.g. PCA) prior to clustering for better performance.</p>

<h4>Collaboration</h4>
<p>In peer activities, we compared clustering techniques and debated the trade-offs between K-Means and hierarchical clustering. I contributed insights on data scaling and shared my Jaccard coefficient implementation during group discussions.</p>

<h4>Evidence</h4>
<p>
GitHub Notebook (private): 
<a href="https://github.com/Jasim766/jasim766.github.io/blob/main/Untitled5.ipynb" target="_blank">Clustering and Jaccard Analysis</a>. 
Notebook includes set-based similarity functions, cluster visualisations, and interpretative plots using Seaborn and Scikit-learn.
</p>

<h4>Skills & Competencies</h4>
<ul>
  <li>Unsupervised learning using clustering algorithms</li>
  <li>Jaccard similarity coefficient and set-based analysis</li>
  <li>Application of K-Means and Agglomerative Clustering</li>
  <li>Evaluation of clusters using silhouette scores and visual diagnostics</li>
</ul>

<hr />


<h2>Unit 6 ‚Äì Clustering with Python</h2>

<h4>Summary of Learning</h4>
<p>This week extended the foundational concepts of clustering by shifting from theory to implementation. Building on the conceptual grounding from Unit 5, we applied the K-Means algorithm to real-world datasets using <code>scikit-learn</code>, one of Python‚Äôs most widely-used machine learning libraries. The emphasis was on understanding how to prepare data, select appropriate values of <code>K</code>, and interpret clustering results in the context of pattern discovery and segmentation. We also explored the practical implications of distance-based clustering on multidimensional data.</p>

<h4>Key Concepts Covered</h4>
<ul>
  <li>Understanding K-Means clustering and its assumptions</li>
  <li>Using scikit-learn for unsupervised learning tasks</li>
  <li>Data preprocessing and feature selection for clustering</li>
  <li>Evaluating clusters using visualisation and label comparison</li>
</ul>

<h4>Formative Activities</h4>
<p>We explored three main tasks across different datasets:</p>
<ol>
  <li><strong>Iris Dataset:</strong> We used <code>K=3</code> to segment the dataset and compared the resulting clusters against actual species labels to assess clustering effectiveness.</li>
  <li><strong>Wine Dataset:</strong> Again using <code>K=3</code>, we tested how well the algorithm could recover known wine classes based solely on numerical features.</li>
  <li><strong>WeatherAUS Dataset:</strong> We experimented with values from <code>K=2</code> to <code>K=6</code> and visualised the groupings using 2D scatter plots to observe structure and overlap between climate patterns.</li>
</ol>

<h4>Reflection (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)</h4>
<p><strong>WHAT:</strong> I learned how K-Means functions as an unsupervised technique, grouping data points by minimising intra-cluster variance. I applied this understanding by experimenting with real datasets of varying structure and scale.</p>
<p><strong>SO WHAT:</strong> This practical experience clarified the importance of choosing the right features and scaling techniques prior to clustering. It also revealed how cluster interpretability can be challenging when labels are not directly observable.</p>
<p><strong>NOW WHAT:</strong> I plan to extend this learning by applying clustering techniques to customer segmentation and anomaly detection in larger business-oriented datasets. Additionally, I will explore other clustering approaches such as DBSCAN or hierarchical clustering for comparison.</p>

<h4>Collaboration</h4>
<p>I contributed to the team project by writing the introduction for our K-Means analysis report. Our discussions focused on choosing the right number of clusters and validating our results against known labels. Peer insights helped refine our preprocessing steps and enhance the overall quality of the clustering output.</p>

<h4>Evidence</h4>
<p>GitHub repository with notebook submissions: <a href="https://github.com/Jasim766/jasim766.github.io/blob/main/Unit06%20K-Means%20demo%20notebook.ipynb" target="_blank">https://github.com/Jasim766/jasim766.github.io/blob/main/unit6_kmeans.ipynb</a>. This includes preprocessing routines, KMeans implementation, evaluation metrics, and visual output for all three datasets.</p>

<h4>Skills & Competencies</h4>
<ul>
  <li>Application of K-Means clustering in real-world scenarios</li>
  <li>Preprocessing large datasets for unsupervised learning</li>
  <li>Evaluation of clustering outcomes using plots and label matching</li>
  <li>Using scikit-learn tools effectively for model development</li>
</ul>

<hr />



<h2>Unit 7 ‚Äì Introduction to Artificial Neural Networks</h2>

<h4>Summary of Learning</h4>
<p>This unit introduced the foundational principles of Artificial Neural Networks (ANNs), focusing on how they are inspired by the structure and function of biological neurons. The content bridged neuroscience concepts with computational implementations, leading to a deeper understanding of how learning systems can adapt based on data. ANNs were presented as key components driving automation, personalization, and predictive analytics across industries‚Äîfrom marketing to robotics.</p>

<h4>Key Concepts Covered</h4>
<ul>
  <li>Structure and function of a biological neuron vs. an artificial neuron</li>
  <li>The architecture of single-layer and multi-layer perceptrons (MLPs)</li>
  <li>Activation functions such as Sigmoid, ReLU, and Tanh</li>
  <li>Learning via weight adjustments using gradient descent</li>
</ul>

<h4>Practical Activities</h4>
<p>Three Jupyter Notebooks guided the practical exploration of neural networks:</p>
<ol>
  <li><code>simple_perceptron.ipynb</code> ‚Äì Implementation of a basic neuron using step functions</li>
  <li><code>perceptron_AND_operator.ipynb</code> ‚Äì A binary logic gate simulation using perceptron logic</li>
  <li><code>multi-layer_perceptron.ipynb</code> ‚Äì A hands-on demo of forward pass and activation flow in a multi-layer model with sigmoid activation</li>
</ol>

<h4>Reflection (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)</h4>
<p><strong>WHAT:</strong> I learned the computational mechanics behind how ANNs process inputs and adjust weights to learn patterns. This included the role of activation functions, error correction, and layer depth in model complexity.</p>
<p><strong>SO WHAT:</strong> The hands-on experience clarified the mathematical foundations of how neural networks converge toward solutions. It also raised awareness of how subtle design choices (like function types or learning rates) influence model performance and interpretability.</p>
<p><strong>NOW WHAT:</strong> I plan to extend this knowledge by implementing more advanced architectures like CNNs and experimenting with backpropagation in TensorFlow or PyTorch. I also intend to explore how explainable AI (XAI) approaches can address the opacity of deep learning systems.</p>

<h4>Collaboration</h4>
<p>This week involved peer discussions around neural network limitations, such as overfitting and vanishing gradients. Feedback loops helped troubleshoot early perceptron implementation errors and compare results using different activation strategies.</p>

<h4>Evidence</h4>
<p>GitHub repository containing all relevant notebooks:</p>
<ul>
  <li>
    <a href="https://github.com/Jasim766/jasim766.github.io/blob/main/Unit07%20Ex1%20simple_perceptron.ipynb" target="_blank">
      Unit07 Ex1 ‚Äì Simple Perceptron
    </a>
  </li>
  <li>
    <a href="https://github.com/Jasim766/jasim766.github.io/blob/main/Unit07%20Ex2%20perceptron_AND_operator.ipynb" target="_blank">
      Unit07 Ex2 ‚Äì Perceptron AND Operator
    </a>
  </li>
  <li>
    <a href="https://github.com/Jasim766/jasim766.github.io/blob/main/Unit07%20Ex3%20multi-layer%20Perceptron.ipynb" target="_blank">
      Unit07 Ex3 ‚Äì Multi-layer Perceptron (Sigmoid)
    </a>
  </li>
</ul>

      
<h4>Skills & Competencies</h4>
<ul>
  <li>Understanding of biological-to-computational neuron mapping</li>
  <li>Development of perceptron-based models using Python</li>
  <li>Use of activation functions and weight adjustments</li>
  <li>Collaboration in debugging and comparing ANN variants</li>
</ul>

<hr />



<h2>Unit 8 ‚Äì Training an Artificial Neural Network</h2>

<h4>Summary of Learning</h4>
<p>
This unit built upon the foundational knowledge of Artificial Neural Networks (ANNs) by focusing on how learning occurs within the network. Specifically, it explored the role of error correction and backpropagation, a critical algorithm that fine-tunes network weights by propagating the error backward through the layers. This learning process allows ANNs to become more accurate over time, making them suitable for a wide range of business applications, from predictive analytics to pattern recognition.
</p>

<h4>Artefact</h4>
<p>
As a practical exercise, I ran the <code>gradient_descent_cost_function.ipynb</code> notebook. This involved visualizing and modifying learning rate and iteration values to observe their influence on the cost function. It clearly demonstrated how model performance improves through iterative weight adjustment, reinforcing the backpropagation concept covered in the lecture.
</p>

<h4>Reflection (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)</h4>
<p><strong>WHAT:</strong> I learned how backpropagation drives ANN learning by updating weights to minimize prediction error.</p>
<p><strong>SO WHAT:</strong> This deepened my understanding of how hyperparameters like learning rate and iteration count affect model convergence. It also made clear why tuning these values is essential for stable training.</p>
<p><strong>NOW WHAT:</strong> Moving forward, I aim to experiment further with network architectures, applying backpropagation to more complex real-world datasets and investigating early stopping and momentum techniques.</p>

<h4>Collaboration</h4>
<p>
We discussed the gradient descent cost function results as a team, comparing our learning rate strategies and visual outputs. This exchange helped refine my approach to choosing effective hyperparameter values based on the shape and slope of the cost function graph.
</p>

<h4>Evidence</h4>
<p>GitHub repository link to my notebook submission:</p>
<ul>
  <li>
    <a href="https://github.com/Jasim766/jasim766.github.io/blob/main/Unit08%20Ex4%20gradient_descent_cost_function.ipynb" target="_blank">
      Unit08 Ex4 ‚Äì Gradient Descent & Cost Function
    </a>
  </li>
</ul>

<h4>Skills & Competencies</h4>
<ul>
  <li>Understanding and implementing backpropagation</li>
  <li>Hyperparameter tuning for learning rate and iterations</li>
  <li>Critical evaluation of model convergence through cost function analysis</li>
</ul>

<hr />



<h2>Unit 9 ‚Äì Introduction to Convolutional Neural Networks</h2>

<h4>Summary of Learning</h4>
<p>
This unit introduced Convolutional Neural Networks (CNNs), a cornerstone of modern computer vision. We explored how CNNs differ from fully connected networks, particularly in their ability to extract spatial features from image data. The unit also discussed more advanced architectures built upon CNNs, including Dueling Networks, GANs, and Transformers. These models have become foundational across various AI domains‚Äîfrom autonomous driving and facial recognition to text generation and image synthesis.
</p>

<h4>Artefact</h4>
<p>
The core activity involved running a pre-trained CNN model for object recognition using the <code>Convolutional Neural Networks (CNN) - Object Recognition.ipynb</code> notebook. I modified the input image index within the line <code>plt.imshow(x_test[16])</code> to test different predictions and observe the model‚Äôs generalisation capabilities. This hands-on task allowed me to visualise how CNN layers process images and make classification decisions.
</p>

<h4>Reflection (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)</h4>
<p><strong>WHAT:</strong> I learned how convolutional layers, pooling layers, and fully connected layers interact to process image data hierarchically.</p>
<p><strong>SO WHAT:</strong> Understanding how these networks work at a structural level has helped me demystify how AI can ‚Äúsee‚Äù and interpret visuals, with direct implications for fields like healthcare diagnostics, surveillance, and augmented reality.</p>
<p><strong>NOW WHAT:</strong> I plan to explore transfer learning with CNNs by using pre-trained models such as VGG16 and ResNet, applying them to domain-specific classification problems in upcoming projects.</p>

<h4>Collaboration</h4>
<p>
In our group discussion, we compared model prediction outcomes for different test images and debated how input variation affected the network‚Äôs accuracy. We also reflected on the ethical implications of CNN-driven technologies, referencing the Wall (2019) article and discussing the societal consequences of facial recognition and surveillance systems.
</p>

<h4>Evidence</h4>
<p>GitHub repository link to my notebook submission:</p>
<ul>
  <li>
    <a href="https://github.com/Jasim766/jasim766.github.io/blob/main/Unit09%20Ex1%20Convolutional%20Neural%20Networks%20(CNN)%20-%20Object%20Recognition.ipynb" target="_blank">
      Unit09 Ex1 ‚Äì CNN Object Recognition Notebook
    </a>
  </li>
</ul>

<h4>Skills & Competencies</h4>
<ul>
  <li>Hands-on application of CNNs for image classification</li>
  <li>Model debugging and prediction interpretation</li>
  <li>Understanding ethical issues in AI-powered vision systems</li>
</ul>

<hr />



<h2>Unit 10 ‚Äì Natural Language Processing (NLP)</h2>

<h4>Summary of Learning</h4>
<p>
This unit explored the foundations and advancements in Natural Language Processing (NLP), a field that bridges linguistics and machine learning. We traced the evolution from early rule-based systems to the introduction of deep learning and transformer-based architectures such as BERT, GPT, and T5. These technologies have transformed how machines understand and generate human language, with real-world applications including chatbots, summarisation, search engines, and automated translation.
</p>

<h4>Artefact</h4>
<p>
The artefact for this unit was a series of activities focused on understanding the impact of transformer architectures and their evaluation. Key metrics such as BLEU, ROUGE, and perplexity were examined for assessing model output. While we did not run specific notebooks, I explored NLP evaluation using public datasets and APIs, adjusting parameters to observe changes in output fluency and coherence.
</p>

<h4>Reflection (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)</h4>
<p><strong>WHAT:</strong> I developed an understanding of how transformers like BERT and GPT use self-attention mechanisms to generate contextual embeddings of language.</p>
<p><strong>SO WHAT:</strong> This allowed me to appreciate how transformer models outperform traditional sequence-based models like LSTMs, especially in tasks requiring contextual understanding. I also learned how essential evaluation metrics are in determining output quality.</p>
<p><strong>NOW WHAT:</strong> I plan to apply transfer learning using HuggingFace transformers in future NLP projects. I also aim to explore fine-tuning and hyperparameter optimisation to improve model generalisation across tasks such as classification, summarisation, and QA.</p>

<h4>Collaboration</h4>
<p>
During our team discussions, we analysed transformer model outputs and compared them to baseline models across different text datasets. This peer review helped identify where models produced hallucinated outputs or lacked syntactic fluency. The seminar included discussions on MLOps best practices for monitoring NLP pipelines and managing model drift in production environments.
</p>

<h4>Evidence</h4>
<p>
Although there was no specific Jupyter notebook required for this week, I experimented with BERT-based summarisation using HuggingFace and evaluated summaries using ROUGE-L and BLEU scores. 
</p>

<h4>Skills & Competencies</h4>
<ul>
  <li>Understanding of Transformer-based models (BERT, GPT, T5)</li>
  <li>Application of BLEU, ROUGE, and perplexity in NLP evaluation</li>
  <li>Introduction to MLOps concepts in the NLP lifecycle</li>
  <li>Hyperparameter tuning and transfer learning in NLP tasks</li>
</ul>

<hr />



<h2>Unit 11 ‚Äì Model Selection and Evaluation</h2>

<h4>Summary of Learning</h4>
<p>
This unit focused on the critical stages of model selection, evaluation, and deployment within the machine learning workflow. Choosing the right model is only the first step‚Äîequally important are evaluating its performance and ensuring it is production-ready through MLOps. This week introduced the practical challenges of deploying ML models, including the need for continuous monitoring, retraining, and scalable infrastructure. We also explored how hyperparameter tuning and metric-driven optimisation can drastically improve a model‚Äôs reliability and generalisation.
</p>

<h4>Artefact</h4>
<p>
The Jupyter notebook <code>model_Performance_Measurement.ipynb</code> was used to experiment with key evaluation metrics such as R¬≤ and AUC (Area Under the Curve). I modified parameters including learning rate, tree depth, and number of estimators to examine their effects on model generalisation. The artefact provides visual outputs and statistical summaries that illustrate how minor tuning can significantly influence performance metrics.
</p>

<h4>Reflection (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)</h4>
<p><strong>WHAT:</strong> I learned how to systematically evaluate ML models using metrics suited for both classification (e.g., AUC, precision-recall) and regression (e.g., R¬≤, MAE). I also explored how to apply grid search and randomised search for hyperparameter tuning.</p>
<p><strong>SO WHAT:</strong> This has helped me shift from relying solely on accuracy to a more holistic view of model evaluation. I now understand when to use specific metrics depending on the task (e.g., AUC for imbalanced classification).</p>
<p><strong>NOW WHAT:</strong> Going forward, I will implement automated pipelines for evaluation and tuning as part of my MLOps approach. This includes monitoring live model performance using versioned models and CI/CD frameworks such as MLflow or SageMaker.</p>

<h4>Collaboration</h4>
<p>
During group discussions, we shared our results from altering model hyperparameters and compared how different optimisation strategies impacted AUC and R¬≤. We also explored the ethical implications of deploying underperforming models in real-world scenarios and how MLOps can mitigate such risks through retraining protocols.
</p>

<h4>Evidence</h4>
<p>
GitHub Repository : 
<a href="https://github.com/Jasim766/jasim766.github.io/blob/main/Unit11_model_Performance_Measurement.ipynb" target="_blank">
model_Performance_Measurement.ipynb</a> ‚Äì Includes multiple experiments with regression and classification models, annotated with observations on tuning outcomes and performance changes.
</p>

<h4>Skills & Competencies</h4>
<ul>
  <li>Hyperparameter tuning using grid and randomised search</li>
  <li>Evaluation of models using R¬≤, AUC, precision, recall, and confusion matrices</li>
  <li>Understanding and applying MLOps principles for robust deployment</li>
  <li>Collaborative model analysis and ethical considerations in deployment</li>
</ul>

<hr />

<h2>Unit 12 ‚Äì Industry 4.0 and Machine Learning</h2>

<h4>Summary of Learning</h4>
<p>
This final unit explored the intersection of emerging machine learning technologies and the ongoing evolution of industrial systems, commonly termed as Industry 4.0. The concept revolves around the integration of intelligent systems, automation, and real-time data into manufacturing and service operations. The discussion extended to Industry 5.0, highlighting the role of human-centric, sustainable innovation.
</p>
<p>
Key technologies such as Self-Supervised Learning (SSL), Edge AI, and Neural Architecture Search (NAS) were introduced as drivers of future intelligent systems. These advances move machine learning beyond traditional models and open new avenues for low-latency, real-time applications in sectors like smart cities, healthcare, autonomous vehicles, and digital manufacturing.
</p>

<h4>Artefact</h4>
<p>
The seminar activity focused on mapping machine learning capabilities to Industry 4.0 frameworks. We discussed real-world case studies where AI-driven automation enhances productivity, reduces human error, and promotes scalability. As part of the formative exercise, we also explored the concept of digital twins‚Äîvirtual replicas of physical systems‚Äîand their reliance on ML for predictive maintenance and decision-making.
</p>

<h4>Reflection (WHAT ‚Üí SO WHAT ‚Üí NOW WHAT)</h4>
<p><strong>WHAT:</strong> I explored the paradigm shift brought about by AI, automation, and smart technologies within industry. I learned about novel ML deployment techniques like Edge AI and their critical role in enabling autonomous decision-making in resource-constrained environments.</p>
<p><strong>SO WHAT:</strong> This understanding reshaped my perspective on how machine learning isn‚Äôt just a tool for data science, but a foundational component of modern infrastructure. The seminar highlighted the urgency for ethical frameworks and sustainability in ML deployment as we progress towards Industry 5.0.</p>
<p><strong>NOW WHAT:</strong> I intend to delve deeper into real-time ML systems and apply this knowledge to future research or work in areas such as digital twins, AI for IoT, and adaptive learning systems. This unit inspired me to pursue projects that blend algorithmic efficiency with societal value.</p>

<h4>Collaboration</h4>
<p>
Team discussions in this unit revolved around mapping ML concepts onto real industrial use cases. I contributed insights on the role of digital twins in smart manufacturing and participated in debates around balancing automation with human oversight in Industry 5.0.
</p>

<h4>Evidence</h4>
<p>
Seminar Notes & Reading Reflections (internal submission). Diagrams linking ML workflows with Industry 4.0 components are documented in the team workspace. Additional artefacts will be uploaded as part of the final reflective e-portfolio submission.
</p>

<h4>Skills & Competencies</h4>
<ul>
  <li>Understanding Industry 4.0 and Industry 5.0 ecosystems</li>
  <li>Knowledge of emerging ML techniques: SSL, Edge AI, NAS</li>
  <li>Application of ML in real-time, low-latency environments</li>
  <li>Critical thinking on ethical and sustainable AI deployment</li>
</ul>

<hr />



      <h4>üìö References</h4>
      <p>[Add APA/Harvard formatted sources used across all units and reflection]</p>
    </div>
  </section>

  <footer>
    <div class="container">
      <p>¬© 2025 Jasim Alzaabi</p>
    </div>
  </footer>
</body>
</html>
